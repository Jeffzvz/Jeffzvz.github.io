---
title: ã€ğŸ˜¶DETRç³»åˆ—ã€‘DETR
link:  End-to-End Object Detection with Transformers
Journal/Conference : ECCV2021
date: 2024-07-28
tags: ["Object Detection","DETRç³»åˆ—"]
---

# 1 æ€»ä½“ï¼ˆå¤ªé•¿ä¸çœ‹ç‰ˆï¼‰

Â Â Â Â DETRçš„æ¨¡å‹åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼šCNN backboneæå–å›¾åƒç‰¹å¾ã€transformerå¢å¼ºå›¾åƒç‰¹å¾ã€FFNåˆ©ç”¨å›¾åƒç‰¹å¾è¿›è¡Œé¢„æµ‹ã€‚DETRå°†ç›®æ ‡æ£€æµ‹è§†ä½œæ˜¯é›†åˆé¢„æµ‹é—®é¢˜ï¼ˆset prediction problems)ï¼Œæ‘’å¼ƒäº†ä¼ ç»Ÿæ–¹æ³•ä¸­æ‰‹å·¥è®¾è®¡éƒ¨åˆ†ï¼ˆå¦‚åŒºåŸŸæè®®ç½‘ç»œRPNã€éæå¤§å€¼æŠ‘åˆ¶NMSç­‰ï¼Œidkå•Šæˆ‘ä¸æ‡‚å•Š~~ï¼‰ã€‚

Â Â Â Â DETRä½¿ç”¨äºŒåˆ†å›¾åŒ¹é…ï¼Œè®¾è®¡äº†ä¸€ä¸ªlossï¼Œå¹¶ä¸”è®¾è®¡äº†object queryçš„embeddingæ§åˆ¶ç”Ÿæˆé¢„æµ‹çš„æ•°ç›®ã€‚å°†æ¯ä¸€ä¸ªé¢„æµ‹çš„è¾¹ç•Œæ¡†ï¼ˆclassï¼Œboxesï¼‰ä¸å”¯ä¸€ä¸€ä¸ªçœŸå®ç‰©ä½“æ¡†ç›¸å¯¹åº”ï¼Œé€šè¿‡è¿™ç§æ–¹æ³•DETRå¯ä»¥å®ç°ç«¯åˆ°ç«¯çš„æ£€æµ‹æ¡†æ¶ï¼ˆä»€ä¹ˆå«é‡‘é‡å•Š~~ï¼‰ã€‚ä¸”DETRç»“åˆäº†å½“ä¸‹æœ€æµè¡Œçš„transformeræ¶æ„ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿä½¿æ¨¡å‹æ›´å…¨é¢äº†è§£åˆ°å›¾åƒçš„ä¸Šä¸‹æ–‡å’Œå…¨å±€å…³ç³»ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„æ£€æµ‹æ•ˆæœã€‚

![](../assets/2024-07-28-16-27-43-image.png)

# 2 DETRçš„Loss

## 2.1 é›†åˆé¢„æµ‹Loss

**å‰æ**ï¼šå‡è®¾æˆ‘ä»¬çŸ¥é“DETRæœ‰Nä¸ªobject queriesï¼ˆN=100ï¼‰ï¼Œå¯ä»¥é¢„æµ‹å¾—åˆ°100ä¸ªè¾¹ç•Œæ¡†ï¼Œè€Œæˆ‘ä»¬çš„GTç‰©ä½“ä¹Ÿæœ‰Nä¸ªï¼ˆå¯èƒ½çœŸå®çš„GTåªæœ‰å‡ ä¸ªï¼Œä½†æœ€åè¢«å¡«å……åˆ°Nä¸ªï¼Œå¡«å……çš„ç±»åˆ«å³ä¸º"no object"ï¼‰ã€‚

ï¼ˆé‚£ä¹ˆæˆ‘ä»¬æ€ä¹ˆçŸ¥é“å“ªä¸ªé¢„æµ‹çš„è¾¹ç•Œæ¡†æ˜¯å’Œå“ªä¸ªGTæ¡†ä¸€ä¸€å¯¹åº”çš„çš„å‘¢ï¼Ÿï¼‰ç­”æ¡ˆå°±æ˜¯ç”¨äºŒåˆ†å›¾åŒ¹é…ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œå‡å¦‚çŸ©é˜µçš„æ¨ªçºµè½´åˆ†åˆ«æ˜¯ä»»åŠ¡ä»¥åŠå·¥äººï¼Œå…¶ä¸­çš„å€¼æ˜¯æ—¶é—´ï¼Œæ¯ä¸ªå·¥äººåªèƒ½å»åšä¸€é¡¹ä»»åŠ¡ä¸”ä¸åŒçš„å·¥äººå¿…é¡»åšä¸åŒçš„ä»»åŠ¡ï¼Œé‚£ä¹ˆå¦‚ä½•åˆ†é…å·¥äººå’Œä»»åŠ¡æ‰èƒ½ä½¿å¾—æ—¶é—´æˆæœ¬æœ€ä½å°±æ˜¯äºŒåˆ†å›¾åŒ¹é…è¦åšçš„äº‹å„¿ã€‚æˆ‘ä»¬å¯ä»¥ç”¨æš´åŠ›ç©·ä¸¾ï¼Œä¹Ÿå¯ä»¥ç”¨å…¶ä»–ç®—æ³•ï¼Œæ­¤å¤„è®ºæ–‡ä½¿ç”¨äº†<mark>åŒˆç‰™åˆ©ç®—æ³•</mark>ã€‚å…¶ä¸­ä»»åŠ¡å’Œå·¥äººåˆ†åˆ«æ˜¯GTå’Œé¢„æµ‹ï¼Œè€ŒçŸ©é˜µçš„å€¼åˆ™æ˜¯è®ºæ–‡è®¾å®šçš„é›†åˆé¢„æµ‹lossã€‚

å…ˆæ¢³ç†ä¸€ä¸‹æ•°æ®å‰æï¼š

$$
\begin{aligned}
y_i &= <c,b_i> \text{ä¸ºçœŸå®è¾¹ç•Œæ¡†çš„ç±»è¢«å’Œboxes}\\
\hat{y_i}&=<\hat c,\hat{b_i}>\text{ä¸ºé¢„æµ‹è¾¹ç•Œæ¡†çš„ç±»è¢«å’Œboxes}\\
b_i &=<x,y,h,w>\in[0,1]\text{ è¡¨ç¤ºboxesçš„ä¸­å¿ƒåæ ‡å’Œç›¸è¾ƒäºåŸå›¾å°ºå¯¸çš„Hã€Wæ¯”ä¾‹,å‡å½’ä¸€åŒ–}\\
p &\text{ ä¸ºé¢„æµ‹çš„ç±»åˆ«ç»è¿‡softmaxåå¾—åˆ°çš„æ¦‚ç‡}
\end{aligned}
$$

é‚£ä¹ˆé›†åˆé¢„æµ‹losså°±ä¸ºï¼š

$$
\mathcal{L}_{\text{match}}(y, \hat{y}) = -\mathbb{1}_{\{c_i\not = \emptyset \}}\hat{p}_{\hat{\sigma}(i)}(c_i) + \mathbb{1}_{\{c_i \neq \emptyset\}} \mathcal{L}_{\text{box}}(b_i, \hat{b}_{\hat{\sigma}(i)})


$$

è€Œæˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯æ‰¾åˆ°è¿™æ ·ä¸€ä¸ªmatcher $\hat{\sigma}$ ä½¿å¾—æœ€åcost matrixçŸ©é˜µé‡Œçš„é›†åˆé¢„æµ‹Lossæ€»å’Œæœ€å°ã€‚

$$
\hat{\sigma} = \underset{\sigma \in \mathfrak{S}_N}{\arg\min} \sum_{i=1}^{N} \mathcal{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)})
$$

ç†è§£å°±æ˜¯ï¼š å›¾åƒä¸­çš„objectsä¸ä¸ºno_objectæ—¶ï¼Œå°±æŠŠç›¸å¯¹åº”çš„é¢„æµ‹ç±»åˆ«æ¦‚ç‡æ‹¿å‡ºã€‚åŸæœ¬æ˜¯$1-p$ï¼Œä½†æ˜¯$\mathbb{1}$æ˜¯å¸¸æ•°ä¸å½±å“matchingæ•…æ­¤å¤„ä¸è¦ã€‚ç„¶åè®¡ç®—å¯¹åº”çš„boxesçš„L1æŸå¤±å’ŒGIoUæŸå¤±ã€‚æ³¨æ„è¿™é‡Œä½¿ç”¨$p$è€Œä¸ä½¿ç”¨å…¶å¯¹æ•°å½¢å¼ï¼Œæ˜¯ä¸ºäº†å’Œåé¢çš„bounding boxæŸå¤±çš„åº¦é‡ä¿æŒä¸€è‡´ã€‚

## 2.2 é›†åˆé¢„æµ‹Lossæºç è§£è¯»

`matcher.py/class HungarianMatcher`

```python
## class HungarianMatcher(nn.Module)çš„forwardå‡½æ•°
@torch.no_grad()
    def forward(self, outputs, targets):

        bs, num_queries = outputs["pred_logits"].shape[:2]  # outputsåŒ…å«ä¸¤ä¸ªkeys:pred_logitså’Œpred_boxes
                                                            # å°ºå¯¸åˆ†åˆ«ä¸º(bs, num_queries, num_classes) ; (bs, num_queries, 4) 
        # We flatten to compute the cost matrices in a batch
        out_prob = outputs["pred_logits"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]
        out_bbox = outputs["pred_boxes"].flatten(0, 1)  # [batch_size * num_queries, 4]

        # Also concat the target labels and boxes
        tgt_ids = torch.cat([v["labels"] for v in targets]) # targetæ˜¯ä¸ªåˆ—è¡¨,æœ‰bsä¸ªå€¼ï¼Œå€¼ä¸ºå­—å…¸(keyä¸ºä¹‹å‰è®¾è®¡çš„annotation),å–boxeså’Œlabels
        tgt_bbox = torch.cat([v["boxes"] for v in targets]) # æ³¨æ„å¦‚æœä¸€å¼ imageé‡Œæœ‰2ä¸ªobjectï¼Œé‚£ä¹ˆboxeså’Œlabelsä¹Ÿå°±æœ‰2ä¸ªå€¼ï¼Œåˆ†åˆ«è¡¨ç¤ºè¿™ä¸¤ä¸ªobjects
                                                            # catåœ¨ä¸€èµ·,å‡å¦‚ä¸€å…±æœ‰6ä¸ªobjectsï¼Œé‚£ä¹ˆidsé•¿åº¦ä¸º[6]ï¼Œbboxé•¿åº¦ä¸º[6,4]
        # Compute the classification cost. Contrary to the loss, we don't use the NLL,
        # but approximate it in 1 - proba[target class].
        # The 1 is a constant that doesn't change the matching, it can be ommitted.
        cost_class = -out_prob[:, tgt_ids]  # å‡è®¾è¯¥batchæœ‰å…±æœ‰6ä¸ªobjects,ä¸‹é¢åŒï¼Œcost_classå°ºå¯¸ä¸º[bs * num_queries, 6]

        # Compute the L1 cost between boxes
        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)    # è®¡ç®—L1è·ç¦»ï¼Œè¿”å›å°ºå¯¸ä¸º[bs * num_queries , 6]

        # Compute the giou cost betwen boxes
        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))

        # Final cost matrix
        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou  # [200,6]
        C = C.view(bs, num_queries, -1).cpu()   # [bs,num_queries,6]

        sizes = [len(v["boxes"]) for v in targets]  # sizeså°ºå¯¸ä¸º[bs]ï¼Œè¡¨ç¤ºæ¯ä¸€å¼ å›¾æœ‰å¤šå°‘ä¸ªobjectsï¼Œæ­¤å¤„ä¸º[2,4]
        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]   # å°†Cæ²¿æœ€åä¸€ç»´ï¼Œæ‹†åˆ†æˆ[bs,num_queries,2], [bs,num_queries,4]
        # è¿”å›çš„indicesä¸º[(array([22, 70], dtype=int64), array([0, 1], dtype=int64)), (array([ 7, 33, 34, 69], dtype=int64), array([0, 1, 2, 3], dtype=int64))]
        # å¯ä»¥ç”¨ç†è§£ä¸ºç¬¬22ä¸ªobject queryå¯¹åº”ç¬¬ä¸€ä¸ªå›¾åƒçš„ç¬¬0ä¸ªobject
        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]
```

![](../assets/2024-07-28-20-38-08-image.png)

``å…³äºGIoU Loss``

```python
import torch
from torchvision.ops.boxes import box_area


def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(-1)   #xä¸º[200,4]ï¼Œæœ€åä¸€ç»´unbindï¼Œå¾—åˆ°4ä¸ª[200]
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]  # å››ä¸ªè§’åæ ‡è®¡ç®—(å·¦ä¸Šè§’å’Œå³ä¸‹è§’)
    return torch.stack(b, dim=-1)   #stackå›å»


# modified from torchvision to also return the union
def box_iou(boxes1, boxes2):
    area1 = box_area(boxes1)    #[200]
    area2 = box_area(boxes2)    #[6]

    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [bs*num_queries, 6, 2]
    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # æ¯”è¾ƒbs*num_queriesä¸ªé¢„æµ‹æ¡†å’Œ 6ä¸ªGTæ¡†çš„æœ€å¤šå€¼,æ¯”å¦‚[1,6,2]æŒ‡çš„æ˜¯ç¬¬ä¸€ä¸ªé¢„æµ‹æ¡†å’Œæ‰€æœ‰GTæ¡†çš„å·¦ä¸Šè§’æœ€å¤§å€¼/å³ä¸‹è§’æœ€å°å€¼

    wh = (rb - lt).clamp(min=0)  # äº¤é›†åŒºåŸŸå†…éƒ¨çš„wå’Œh
    inter = wh[:, :, 0] * wh[:, :, 1]  # [bs*num_queries, 6]ï¼Œè¡¨ç¤ºäº¤é›†é¢ç§¯

    union = area1[:, None] + area2 - inter # [bs*num_queries, 6], å¹¶é›†åŒºåŸŸé¢ç§¯,

    iou = inter / union # iou = äº¤é›†é¢ç§¯/å¹¶é›†é¢ç§¯
    return iou, union


def generalized_box_iou(boxes1, boxes2):
    """
    Generalized IoU from https://giou.stanford.edu/

    The boxes should be in [x0, y0, x1, y1] format

    Returns a [N, M] pairwise matrix, where N = len(boxes1)
    and M = len(boxes2)
    """
    # degenerate boxes gives inf / nan results
    # so do an early check
    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()   # ç°åœ¨çš„boxesæ˜¯å·¦ä¸Šè§’åæ ‡ï¼ˆè¾ƒå°ï¼‰å’Œå³ä¸‹è§’åæ ‡ï¼ˆè¾ƒå¤§ï¼‰
    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()
    iou, union = box_iou(boxes1, boxes2)

    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])
    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])

    wh = (rb - lt).clamp(min=0)  # [N,M,2]
    area = wh[:, :, 0] * wh[:, :, 1]

    return iou - (area - union) / area  # giou = iou - (å¹¶é›†-äº¤é›†)/å¹¶é›†
```

## 2.3 åŒˆç‰™åˆ©LossåŠæºç 

Losså…¬å¼å¦‚ä¸‹ï¼š

$$
\mathcal{L}_{\text{Hungarian}}(y, \hat{y}) = \sum_{i=1}^{N} \left[-\log \hat{p}_{\hat{\sigma}(i)}(c_i) + \mathbf{1}_{\{c_i \neq \varnothing\}} \mathcal{L}_{\text{box}}(b_i, \hat{b}_{\hat{\sigma}(i)})\right]
$$

- æ­¤å¤„éœ€è¦æ³¨æ„çš„æ˜¯è¿™é‡Œçš„label lossä¸åœ¨åªæ˜¯å¯¹éâ€œno object"åšï¼Œè®¡ç®—label lossä¹‹å‰ä¼šå»å¡«å……targetï¼Œå¡«å……å€¼ä¸º91è¡¨ç¤º"no object"ï¼Œä½¿ä¹‹å’Œé¢„æµ‹boxesçš„å°ºå¯¸ä¸€æ ·[bs,num_queries]ã€‚

```python
Â Â Â Â Â Â Â Â num_boxes = sum(len(t["labels"]) for t in targets)  # [num_objects]ï¼Œæ­¤å¤„ä¸º[6]
        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)
        if is_dist_avail_and_initialized():
            torch.distributed.all_reduce(num_boxes)
        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item() # æ£€æŸ¥æ¯ä¸ªgpuä¸Šçš„boxesæ˜¯å¦å°äº1

        # Compute all the requested losses
        losses = {}
        for loss in self.losses:
            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))
```

- label loss:

```python
    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):
        """Classification loss (NLL)
        targets dicts must contain the key "labels" containing a tensor of dim [nb_target_boxes]
        """
        assert 'pred_logits' in outputs
        src_logits = outputs['pred_logits'] #[bs, num_queries, num_classes]

        idx = self._get_src_permutation_idx(indices)    # è¿”å›tuple,ç¬¬ä¸€ä¸ªå€¼ä¸ºå›¾åƒç´¢å¼•ï¼Œæ—¢å“ªä¸ªobj queryå¯¹åº”å“ªå¼ å›¾åƒï¼›ç¬¬äºŒä¸ªå€¼ä¸ºobj queryç´¢å¼•
        target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(targets, indices)])   # Jä¸ºè¯¥å›¾åƒå¯¹åº”çš„objectç´¢å¼•,å¾—åˆ°æ¯ä¸ªobjectå¯¹åº”çš„target labels
        target_classes = torch.full(src_logits.shape[:2], self.num_classes,
                                    dtype=torch.int64, device=src_logits.device)    # [bs,num_queries]å¡«æ»¡å€¼ä¸ºnum_classes(æ­¤å¤„ä¸º91)
        target_classes[idx] = target_classes_o  #æ­¤å¤„è¡¨ç¤ºåœ¨ç¬¬iä¸ªimageçš„ç¬¬jä¸ªqueryå¡«å……å¯¹åº”çš„target_classes_oï¼Œæ­¤æ­¥éª¤ä¸ºå¡«å……"no obj"

        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)  # [bs,num_query,num_classes] -> [bs,num_classes,num_query]
        losses = {'loss_ce': loss_ce}

        if log:
            # TODO this should probably be a separate loss, not hacked in this one here
            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]    #é”™è¯¯ä¸ªæ•°
         return losses

    def _get_src_permutation_idx(self, indices):
        # permute predictions following indices
        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)]) # iä¸ºç¬¬iä¸ªå›¾åƒ,srcä¸ºç¬¬iä¸ªå›¾åƒå¯¹åº”çš„queryçš„ç´¢å¼•,_ä¸ºç¬¬iä¸ªå›¾åƒå¯¹åº”çš„objectç´¢å¼•
                                                                                    # batch_idxå¾—åˆ°[num_object],æ¯”å¦‚[0,0,1,1,1,1]ï¼Œå¯çŸ¥å“ªä¸ªobjectå¯¹åº”å“ªä¸ªå›¾åƒ
        src_idx = torch.cat([src for (src, _) in indices])  # å¦‚tensor([22, 70,  7, 33, 34, 69])ï¼Œæ—¢å¯¹åº”çš„object queryç´¢å¼•
        return batch_idx, src_idx   
```

![](../assets/2024-07-28-22-01-08-image.png)

- box lossï¼š

```python
    def loss_boxes(self, outputs, targets, indices, num_boxes):
        """Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss
           targets dicts must contain the key "boxes" containing a tensor of dim [nb_target_boxes, 4]
           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.
        """
        assert 'pred_boxes' in outputs
        idx = self._get_src_permutation_idx(indices)
        src_boxes = outputs['pred_boxes'][idx]  # [num_object,4]
        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0) #[num_object,4]

        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')

        losses = {}
        losses['loss_bbox'] = loss_bbox.sum() / num_boxes

        loss_giou = 1 - torch.diag(box_ops.generalized_box_iou(
            box_ops.box_cxcywh_to_xyxy(src_boxes),
            box_ops.box_cxcywh_to_xyxy(target_boxes)))
        losses['loss_giou'] = loss_giou.sum() / num_boxes
        return losses
```

## 2.4 å…¶ä»–

è¿˜ä¼šå»è®¡ç®—è¾…åŠ©lossï¼Œå¯ä»¥æé«˜è®­ç»ƒé€Ÿåº¦ä»¥åŠç¨³å®šæ€§ã€‚ä»æºç ä¸­å¯ä»¥çœ‹åˆ°ï¼ŒCE Lossï¼šL1 Lossï¼š GIoU Loss = 1ï¼š5ï¼š2

# 3 DETRÂ æ¨¡å‹

transformeræ¨¡å‹æ¶æ„å¦‚ä¸‹ï¼š

![](../assets/2024-07-28-22-10-42-image.png)

- encoderè¾“å…¥çš„æ˜¯image featuresï¼Œå¹¶ä¸”åœ¨Qã€Kä½ç½®ä¸Šä¸spatial positional encodingç›¸åŠ ï¼ŒåŠ å…¥ä½ç½®ä¿¡æ¯ã€‚ç»è¿‡6å±‚encoderä¹‹åï¼Œè¾“å‡ºå¸¦æœ‰å…¨å±€ç›¸å…³æ€§çš„image featuresï¼Œç§°ä½œæ˜¯memoryï¼ˆencoder outputï¼‰

- decoderçš„ç¬¬ä¸€å±‚è¾“å…¥æ˜¯object queryï¼Œå…ˆå»åšè‡ªæ³¨æ„åŠ›æ“ä½œï¼Œéšåçš„cross attentionä¸­ï¼ŒQä¸ºdecoderçš„è¾“å‡ºï¼Œå¹¶ä¸”åŠ ä¸Šobject queryï¼›Kä¸ºmemoryï¼Œå¹¶ä¸”åŠ ä¸Špositional encodingï¼ŒVä¸ºmemoryã€‚object queryå¯ä»¥è¡¨ç¤ºä¸ºä¸åŒçš„objectsï¼ˆå› ä¸ºä¸€ä¸ªobject queryè¡¨ç¤ºä¸€ä¸ªæ£€æµ‹æ¡†ï¼‰ï¼Œæ­¤å¤„çš„æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æ³¨å…¥ä¸åŒobjectçš„ç›¸å…³æ€§ï¼Œå†åŠ å…¥åˆ°å›¾åƒç‰¹å¾ä¸­ã€‚

å¦‚ä½•æ„å»ºå°±ä¸åˆ†æäº†ï¼Œä¸»è¦å°±æ˜¯backboneã€positional encodingä»¥åŠtransformerï¼Œåé¢å•ç‹¬å†™æ–‡ç« å»å­¦ä¹ ã€‚ä¸»è¦çœ‹å‰å‘è¿‡ç¨‹ã€‚ä½†æ˜¯æ³¨æ„object queryçš„æ„å»ºï¼š

``self.query_embed = nn.Embedding(num_queries, hidden_dim)`` å…¶å®å°±æ˜¯ä¸€ä¸ªquery embedding

## 3.1 å‰å‘è¿‡ç¨‹

å‰å‘è¿‡ç¨‹ç›¸è¾ƒæ¥è¯´æ¯”è¾ƒç®€å•ã€‚

- ç»è¿‡backboneï¼Œå¾—åˆ°image featuresï¼Œå°ºå¯¸ä¸º[bs,2048,h,w], æ³¨æ„maskä¸º[bs,h,w]ï¼Œmaskè¡¨ç¤ºå“ªä¸ªä½ç½®æ˜¯è¢«paddingçš„ï¼Œä»è€Œä¸è®¡ç®—å…¶æ³¨æ„åŠ›ã€‚

- éšåå°†image featuresç»è¿‡1x1å·ç§¯åï¼Œå†å˜å½¢ä¸º[hw,bs,256]ï¼ŒåŒæ—¶å°†position embeddingä¹Ÿå˜å½¢ä¸º[hw,bs,256]ï¼Œå¹¶å°†query embeddingå˜å½¢ä¸º[100,bs,256]

- é€å…¥åˆ°encoderï¼Œå…¶ä¸­qå’Œkè¦åŠ ä¸Šposition embeddingï¼Œvåˆ™ä¸ç”¨ã€‚è¾“å‡ºmemory
  
  ```python
      def forward_post(self,
                       src,
                       src_mask: Optional[Tensor] = None,
                       src_key_padding_mask: Optional[Tensor] = None,
                       pos: Optional[Tensor] = None):
          q = k = self.with_pos_embed(src, pos)   # åŠ ä¸Špos embedding
          src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,
                                key_padding_mask=src_key_padding_mask)[0]
          src = src + self.dropout1(src2)
          src = self.norm1(src)
          src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
          src = src + self.dropout2(src2)
          src = self.norm2(src)
          return src
  ```

- é€å…¥åˆ°decoderä¸­ï¼Œå…¶ä¸­åšself-attentionæ—¶ï¼Œqå’Œkéƒ½è¦åŠ ä¸Šquery embedding,våˆ™ä¸ç”¨ã€‚åšcross attentionæ—¶ï¼Œqåˆ™ä¸ºtgtï¼ˆdecoderè¾“å‡ºï¼‰å’Œquery embeddingç›¸åŠ ï¼Œkåˆ™ä¸ºmemoryå’Œpositional embeddingç›¸åŠ ï¼Œvåˆ™ä¸ºmemoryã€‚
  
  ```python
      def forward_post(self, tgt, memory,
                       tgt_mask: Optional[Tensor] = None,
                       memory_mask: Optional[Tensor] = None,
                       tgt_key_padding_mask: Optional[Tensor] = None,
                       memory_key_padding_mask: Optional[Tensor] = None,
                       pos: Optional[Tensor] = None,
                       query_pos: Optional[Tensor] = None):
          q = k = self.with_pos_embed(tgt, query_pos) # æ­¤å¤„pos embeddingä¸ºquery pos
          tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,
                                key_padding_mask=tgt_key_padding_mask)[0]
          tgt = tgt + self.dropout1(tgt2)
          tgt = self.norm1(tgt)
          tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),
                                     key=self.with_pos_embed(memory, pos),
                                     value=memory, attn_mask=memory_mask,
                                     key_padding_mask=memory_key_padding_mask)[0] # self-attnå’Œmultihead_attnéƒ½æ˜¯MultiHeadAttn
          tgt = tgt + self.dropout2(tgt2)
          tgt = self.norm2(tgt)
          tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
          tgt = tgt + self.dropout3(tgt2)
          tgt = self.norm3(tgt)
          return tgt
  ```

- æœ€åçš„è¾“å‡ºç»è¿‡ä¸€å±‚linear layerå¾—åˆ°ç±»åˆ«é¢„æµ‹ï¼Œç»è¿‡MLPå¾—åˆ°åæ ‡é¢„æµ‹ï¼ˆsigmoidï¼‰

# 4 å®éªŒç»“æœ

![](../assets/2024-07-28-23-35-29-image.png)

- åªå…³æ³¨encoderæœ€åä¸€å±‚ï¼Œå…³æ³¨å‡ ä¸ªç‚¹çš„ä½ç½®ï¼Œå¯è§encoderä¼¼ä¹å·²ç»å¯ä»¥åŒºåˆ†å‡ºä¸åŒçš„instances

![](../assets/2024-07-28-23-44-15-image.png)

- å¯¹äºdecoderï¼Œç”±äºencoderä»¥åŠå¯ä»¥å¤§è‡´åŒºåˆ†ä¸åŒçš„instancesï¼Œdecoderæ›´å¤šæ˜¯å…³æ³¨ç‰©ä½“çš„æç«¯

![](../assets/2024-07-29-00-02-10-image.png)

- å…³äºä½ç½®ç¼–ç çš„æ¶ˆèï¼š
  
  - å¦‚æœåªä½¿ç”¨object queryï¼ŒAPä¸‹é™éå¸¸å¤š
  
  - å¦‚æœåªåœ¨ç¬¬ä¸€å±‚encoderå’Œdecoderä½¿ç”¨positional encodingï¼Œåˆ™å¥½å¾ˆå¤šï¼Œè¯´æ˜positional encodingå¾ˆå¥½ä½¿
  
  - å¦‚æœä½¿ç”¨learned positional encodingï¼Œå¹¶ä¸”åœ¨åœ¨æ•´ä¸ªä¼ ï¼Œä¼šæå‡ä¸€ç‚¹
  
  - å¦‚æœä¸åœ¨encoderä½¿ç”¨positional encodingï¼Œåªåœ¨decoderçš„cross attentionä¸­ä½¿ç”¨ï¼Œä¸‹é™ä¸€ç‚¹
  
  - ç¬¬äº”è¡Œåˆ™æ˜¯baseline

![](../assets/2024-07-29-00-14-04-image.png)

- å…³äºæŸå¤±å‡½æ•°çš„æ¶ˆè

![](../assets/2024-07-29-00-15-28-image.png)

- å…³äºobject queryçš„å¯è§†åŒ–ï¼Œè™½ç„¶è¯´çœ‹ä¸Šå»å„å¸å…¶èŒï¼Œä½†æ˜¯é‡å éƒ¨åˆ†è¿˜æ˜¯è›®å¤§çš„
